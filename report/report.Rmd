---
title: 'Words have meaning: language choice and startup success'
author: "Zachary Hayes, Justin Liu, Mike McCormick"
date: '2022-04-18'
header-includes:
    - \usepackage{setspace}\doublespacing
output:
  pdf_document:
    extra_dependencies: float
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "!H", out.extra = "", fig.width=6, fig.height=3)
```

```{r libraries, echo = F, warning = F, error = F, message = F, results = 'hide' }
library(tidyverse)
library(tm)
library(xml2)
library(rvest)
library(here)
library(dplyr)
library(tidytext)
library(textstem)
library(pdftools)
library(textclean)
library(text2vec)
library(lsa)
library(widyr)
library(kableExtra)
library(sjPlot)
library(wordcloud)
library(broom)
library(urbnmapr)
library(MASS)
```


```{r import data, echo = F, warning = F, error = F, message = F, results = 'hide' }

################################################################################
### scrape company descriptions

# Y Combinator website sitemap
sitemap <- "https://www.ycombinator.com/companies//sitemap.xml"
pattern <- "https://www.ycombinator.com/companies/"

# read sitemap XML and convert into dataframe and vector of company name and urls
company_url <- sitemap %>% read_xml() %>% as_list() %>% 
  as_tibble() %>% unnest_longer(urlset) %>% 
  filter(urlset_id == "loc") %>% 
  unnest(cols = names(.)) %>% unnest(urlset) %>% rename("url" = "urlset") %>% 
  mutate(company_name = gsub("https://www.ycombinator.com/companies/", "", url))

# write data to csv
#write_csv(company_url, file = here("data", "company_url.csv"))

# define paths for company name, season, and status tags
path_name <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[1]/h1"
path_season <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[2]/span[1]/text()"
path_status <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[2]/span[2]"

# scrape company descriptions from Y Combinator website based on url vector
# ycombinator_data <- tibble(directory_name = character(), 
#                            website_name = character(), season = character(), 
#                            status = character(), headline = character(), 
#                            description = character(), url = character())

# for(i in 1:length(company_url$url)){
#   cat("Iteration", i, "out of", length(company_url$url), "\n")
#   page <- read_html(company_url$url[i])
#   directory_name <- company_url$company_name[i]
#   website_name <- page %>% html_nodes(xpath = path_name) %>% html_text()
#   season <- page %>% html_nodes(xpath = path_season) %>% html_text()
#   status <- page %>% html_nodes(xpath = path_status) %>% html_text()
#   headline <- page %>% html_nodes("h3") %>% html_text() %>% .[1]
#   description <- page %>% html_nodes("p") %>% html_text() %>% .[1]
#   ycombinator_data <- ycombinator_data %>%  
#     add_row(directory_name = directory_name, website_name = website_name, 
#             season = season, status = status, headline = headline,
#             description = description, url = company_url$url[i])
# }

# write data to csv

# read previously downloaded data from csv
ycombinator_data <- read_csv(here("data", "ycombinator_data.csv"))

################################################################################
### company location information

# define path for company location
path_location <- "/html/body/div/div[1]/div/div/div[2]/div/div[2]/div[3]/span[2]"

# scrape company locations from Y Combinator website based on url vector
location_data <- tibble(directory_name = character(), location = character())

# for(i in 1:length(company_url$url)){
#    cat("Iteration", i, "out of", length(company_url$url), "\n")
#    page <- read_html(company_url$url[i])
#    directory_name <- company_url$company_name[i]
#    location <- page %>% html_nodes(xpath = path_location) %>% html_text()
#    location_data <- location_data %>%
#      add_row(directory_name = directory_name, location = location)
# }

# write data to csv
#write_csv(location_data, file = here("data", "location_data.csv"))

# read data from csv
location_data <- read_csv(here("data", "location_data.csv"))

# summary table
location_summary <- location_data %>% 
  count(location)

# write data to csv, then manually categorize
#write_csv(location_summary, file = here("data", "location_summary.csv"))

# read manually categorized summary
location_summary <- read_csv(here("data", "location_summary.csv"))

# break down data by country
country_breakdown <- location_summary %>%
  group_by(country) %>% 
  summarize(count = sum(n))

# top 5 countries
top_5_countries <- country_breakdown %>%
  slice_max(count, n = 5) %>% 
  mutate(category = "country") %>% 
  rename(location = country)

# top_5_countries_plot <- ggplot(top_5_countries, aes(x = reorder(country, -count), y = count)) +
#   geom_col() +
#   labs(x = "country", title = "Top five countries where Y Combinator companies are founded")

# break down data by US states
state_breakdown <- location_summary %>%
  filter(country == "USA") %>% 
  group_by(state) %>% 
  summarize(count = sum(n))

# top 5 states
top_5_states <- state_breakdown %>%
  slice_max(count, n = 5) %>% 
  mutate(category = "state") %>% 
  rename(location = state)

# top_5_states_plot <- ggplot(top_5_states, aes(x = reorder(state, -count), y = count)) +
#   geom_col() +
#   labs(x = "state", title = "Top five states where Y Combinator companies are founded")

# combined states and countries data
combined_location <- top_5_states %>% 
  add_row(top_5_countries)

combined_location_plot <- ggplot(combined_location, 
                                 aes(x = reorder(location, -count), y = count)) +
  geom_col() +
  facet_grid(~category, scales = "free_x") +
  labs(title = "Figure 2 - top five locations where companies are founded", x = "",
       y = "number of companies") + 
  theme(axis.text.x = element_text(angle = 45, vjust = .9 , hjust=1))

################################################################################
### state heatmap

# rename tibble to match ubrnmapr format
heatmap_states <- state_breakdown %>% 
  rename(state_abbv = state) 

# read state geo data
us_st <- urbnmapr::states 

# join data
company_st <- left_join(us_st, heatmap_states, by = "state_abbv")
company_st_plot <-  company_st %>%
  ggplot(aes(long, lat, group = group, fill = count)) +
  geom_polygon(color = NA) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(title = "Figure 3 - geographic distribution of Y Combinator companies",
       x = "", y = "", ) +
    theme(axis.title.x=element_blank(), 
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(), 
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())

################################################################################
### import textbook extracts

pdf_list <- list.files(here("data", "textbooks"))
textbooks <- tibble(textbook = character(), text = character())

for(i in 1:length(pdf_list)){
  cat("Iteration", i, "out of", length(pdf_list), "\n")
  textbook <-  pdf_list[i]
  text <- here("data", "textbooks", pdf_list[i]) %>%
    pdf_text() %>% str_flatten()
  textbooks <- textbooks %>% add_row(textbook = textbook,
                                     text = text)
}

# remove extraneous title info and rename columns to match company tibble
textbooks <- textbooks %>% 
  mutate(textbook = textbook %>% 
           str_remove_all("extract_|.pdf")) %>% 
  rename(directory_name = textbook, description = text)

################################################################################
### descriptive statistics

# ycombinator initial data

yco_init <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
    rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  group_by(document) %>% 
  summarize(count_words = n())

yco_init_stats <- yco_init %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "companies, pre-processing") %>% 
  relocate(data)

# ycombinator post-processing

remove <- "[:digit:]|[:blank:]|[:punct:]"

yco_post <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word)) %>% 
  group_by(document) %>% 
  summarize(count_words = n()) 

yco_post_stats <- yco_post %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "companies, post-processing") %>% 
  relocate(data)

# textbook initial data

textbooks_init <- textbooks %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  group_by(document) %>% 
  summarize(count_words = n())

textbooks_init_stats <- textbooks_init %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "textbooks, pre-processing") %>% 
  relocate(data)

# textbooks post-processing

remove <- "[:digit:]|[:blank:]|[:punct:]"

textbooks_post <- textbooks %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word)) %>% 
  group_by(document) %>% 
  summarize(count_words = n()) 

textbooks_post_stats <- textbooks_post %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "textbooks, post-processing") %>% 
  relocate(data)

# combined summary table

descr_stats <- yco_init_stats %>% 
  add_row(yco_post_stats) %>% 
  add_row(textbooks_init_stats) %>% 
  add_row(textbooks_post_stats) 

descr_stats_table <- kbl(descr_stats) %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position") %>% 
  kable_styling(latex_options = "scale_down") %>% 
  add_header_above(c(" " = 1, " " = 1, "words per document" = 7))

# combined histogram

yco_init <- yco_init %>% 
  mutate(data = "before processing")

yco_post <- yco_post %>% 
  mutate(data = "after processing")

yco_init_post <- yco_init %>% add_row(yco_post)
yco_init_post$data <- factor(yco_init_post$data, 
                             levels = c("before processing",
                                        "after processing"))

yco_hist <- yco_init_post %>% 
  ggplot(aes(x = count_words)) +
  geom_histogram() +
  facet_grid(~data) + 
  labs(x = "words per company description", y = "frequency",
       title = "Figure 1 - histogram of words per company description")
 

################################################################################
### combine company descriptions and textbook extracts, initial processing to 
### trim whitespace and NA values

data <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  filter(!is.na(description)) %>%
  filter(description != "") %>% 
  add_row(textbooks) %>% 
  rename(document = directory_name, text = description)

################################################################################
### tidy and clean documents

# unnest words, remove numbers/whitespace/punctuation/blank rows/stopwords,
# lemmatize

remove <- "[:digit:]|[:blank:]|[:punct:]"

data_clean <- data %>% 
  dplyr::select(document, text) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word))

data_clean_companies <- data_clean %>% 
  count(document)

# calculate term frequency per company
tf <- data_clean %>% 
  count(document, word, sort = TRUE)

# calculate total terms per company  
total_words <- tf %>% 
  group_by(document) %>% 
  summarise(total = sum(n))

# join tables together
tf <- left_join(tf, total_words)

# cast into dtm format and convert to matrix
dtm <- tf %>% 
  cast_dtm(document, word, n)
inspect(dtm)

dtm_matrix <- as.matrix(dtm)

# tf-idf table

tf_idf <- tf %>% 
  bind_tf_idf(word, document, n)

tf_idf_sparse <- tf_idf %>% 
  cast_sparse(document, word, tf)

################################################################################
### calculate cosine similarity per textbook and company description

# extract vectors for each textbook
x_ent <- dtm_matrix["entrepeneurship",]
x_fin <- dtm_matrix["finance",]
x_ldr <- dtm_matrix["leadership",]
x_mkt <- dtm_matrix["marketing",]
x_str <- dtm_matrix["strategy",]

# convert to jaccard logical vectors for each textbook
x_ent_j <- x_ent > 0
x_fin_j <- x_fin > 0
x_ldr_j <- x_ldr > 0 
x_mkt_j <- x_mkt > 0
x_str_j <- x_str > 0

# create empty cosine tibble
cosine_table <- tibble(directory_name = character(), ent_c = numeric(), 
                       fin_c = numeric(), ldr_c = numeric(), mkt_c = numeric(),
                       str_c = numeric(), ent_j = numeric(), fin_j = numeric(),
                       ldr_j = numeric(), mkt_j = numeric(), str_j = numeric())

# define jaccard similiary function
jaccard <- function(x, y) {
  intersection = x %*% y
  union = length(x) + length(y) - intersection
  return(intersection/union)
}

# calculate cosine and jaccard similarity for each company and textbook
for(i in 1:length(data_clean_companies$document)){
  cat("Iteration", i, "out of", length(data_clean_companies$document), "\n")
  doc <-  data_clean_companies$document[i]
  y <- dtm_matrix[doc,]
  y_j <- y > 0

  #calculate cosine similarity
  ent_c <- cosine(x_ent, y)
  fin_c <- cosine(x_fin, y)
  ldr_c <- cosine(x_ldr, y)
  mkt_c <- cosine(x_mkt, y)
  str_c <- cosine(x_str, y)

  #calculate jaccard similiary
  ent_j <- jaccard(x_ent_j, y_j)
  fin_j <- jaccard(x_fin_j, y_j)
  ldr_j <- jaccard(x_ldr_j, y_j)
  mkt_j <- jaccard(x_mkt_j, y_j)
  str_j <- jaccard(x_str_j, y_j)

  cosine_table <- cosine_table %>%  
    add_row(directory_name = doc, ent_c = ent_c, fin_c = fin_c,
            ldr_c = ldr_c, mkt_c = mkt_c, str_c = str_c, ent_j = ent_j,
            fin_j = fin_j, ldr_j = ldr_j, mkt_j = mkt_j, str_j = str_j)
}

# remove textbooks from cosine_table and rename columns
remove <- c("entrepeneurship", "finance", "leadership", "marketing", "strategy")

cosine_table <- cosine_table %>% 
  filter(!directory_name %in% remove) %>% 
  mutate(ent_c = ent_c[,1], fin_c = fin_c[,1], ldr_c = ldr_c[,1],
         mkt_c = mkt_c[,1], str_c = str_c[,1])

################################################################################
### import and clean crunchbase funding data

# import funding data csv and add company name column
crunchbase_data <- read.csv(here("data", "CB_funding.csv")) %>% 
  mutate(directory_name = gsub("\\d*_", "", filename)) %>% 
  mutate(directory_name = gsub(".json", "", directory_name))

# filter based on matching name from Y Combinator website, collapse to one row
# per company
funding_data <- crunchbase_data %>% 
  dplyr::select(directory_name, num_funding_rounds) %>% 
  filter(directory_name %in% company_url$company_name) %>% 
  group_by(directory_name, num_funding_rounds) %>% 
  summarise(n = n_distinct(directory_name)) %>% 
  rename(funded = n)

################################################################################
### create dummy variables for exit status from ycombinator data

exit <- ycombinator_data %>% 
  dplyr::select(directory_name, status) %>% 
  mutate(acquired = ifelse(status == "Acquired", 1, 0)) %>% 
  mutate(active = ifelse(status == "Active", 1, 0)) %>% 
  mutate(inactive = ifelse(status == "Inactive", 1, 0)) %>% 
  mutate(public = ifelse(status == "Public", 1, 0))

################################################################################
### join similarity, funding, and exit tables

# join cosine table with funding data
join1 <- left_join(cosine_table, funding_data, by = "directory_name")

# join status table (exit info)
data_joined <- left_join(join1, exit, by = "directory_name")

# replace NA values with 0
data_joined$num_funding_rounds <- data_joined$num_funding_rounds %>% 
  replace_na(0)
data_joined$funded <- data_joined$funded %>% 
  replace_na(0)

################################################################################
### funding data statistics

funding_summary <- data_joined %>% 
  summarize(total = n(),
            active = sum(active),
            funded = sum(funded),
            inactive = sum(inactive),
            acquired = sum(acquired),
            public = sum(public)) %>% 
  pivot_longer(everything(), names_to = "status", values_to = "count")

funding_summary_plot <- ggplot(funding_summary, 
                               aes(x = reorder(status, -count), y = count)) +
  geom_bar(stat = "identity") +
  labs(x = "status", y = "number of companies",
       title = "Figure 4 - company status statistics")


################################################################################
### regression analysis

# ordinal logistic regression of company status

data_polr <- data_joined
data_polr$status <- factor(data_polr$status, 
                           levels = c("Inactive", "Active", "Acquired", "Public"), ordered = TRUE)

log_status <- polr(status ~ ent_c + fin_c + ldr_c + mkt_c + str_c + ent_j + fin_j +
                ldr_j + mkt_j + str_j, data = data_polr, Hess = TRUE)

# multivariable logistic regression for funding

# funded multivariable logistic regression
log_funded <- glm(formula = funded ~ ent_c + fin_c + ldr_c + mkt_c + str_c +
                    ent_j + fin_j + ldr_j + mkt_j + str_j,
                  data = data_joined, family = "binomial"(link = "logit"))

```

## **Introduction**

text text text

### Motivation

text text text

### Background

text text text

### Hypothesis

text text text

## **Data**

### Y Combinator startup company descriptions

text text text

```{r company data, message = F, echo = F}
yco_hist 
```

text text text

```{r location data, message = F, echo = F}
combined_location_plot 
```

text text text

```{r state data, message = F, echo = F}
company_st_plot
```

### Open source textbook extracts

text text text


**Table 1 - textual data summary statistics**
```{r textual data, message = F, echo = F}
descr_stats_table
```

### Crunchbase startup funding data

text text text

**Table 2 - funding and exit data for Y Combinator companies**
```{r funding, message = F, echo = F}

funding_stats_table <- kbl(funding_summary) %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position", position = "left") 
funding_stats_table

funding_summary_plot

```

## **Methodology**

Once we obtained the required data, our methodology followed three main steps: 
(1) processing the data into useable form, (2) calculating cosine and Jaccard 
similarity scores between each company description and each textbook, and (3)
use ordered and multivariate regressions to analyse the relationship between
the calculated similarity scores and company outcomes. These steps are discussed
in detail in the following sections.

#### Textual data

Our two separate sources of textual data required different means to extract and
then process into usable formats. We extracted the company descriptions using 
the rvest package, looping through each company's individual website and scraping 
the required information. For the five textbooks, we manually extracted just the 
table of contents, index, or glossary (as available, it varied per textbook) and
then imported them into R using the pdftools package.

The initial textual data, once imported into R, was still not in a format useful
for analysis. We followed several steps to convert the data into a usable format:

(1) We removed all numbers, whitespace, punctuation, and any blank or "NA"
rows to reduce the data to just words. 

(2) Next, we "lemmatized" each word using the textstem::lemmatize_words
function. This function uses a dictionary based on the Mechura 2016 English 
lemmatization list. This step reduced the words to common base forms, reducing the 
complexity of the data and making it easier to analyze.

(3) Finally, we calculated term frequencies for each document, and then used this 
information to build a document-term matrix of all the company descriptions, 
textbook extracts, and terms. The matrix contains a set of vectors for each textbook
extract and company description, with each value corresponding to a specific term
and the frequency it is found in each document.

#### Funding and exit data

We based the current status of each company based on information scraped from the 
Y Combinator website - each company is described as either "Inactive" (gone out
of business), "Active", "Acquired", or "Public". We were fortunate in this project
in that the Y Combinator and Crunchbase websites use a similar naming convention 
for companies, making joining the textual data with the funding data an easy step.
We filtered the Crunchbase funding data down to the company name and the number 
of funding rounds received and joined this data to the list of companies using 
the dplyr::left_join function. The Crunchbase data only included companies that 
have received startup funding, enabling us to define a second logical variable 
for whether or not a company had received funding.

#### Similarity scores

The final processing step required was to calculate the similarity between each 
company description and the textbook extracts, i.e. how similar a company's 
initial descriptive language is to a business topic. We did this using two measures, 
the cosine and Jaccard similarity scores between each company description and 
each textbook. 

We calculated the cosine similarity score for each company description using the
lsa::cosine function. This function takes two arguments, the respective vectors 
for each company description and textbook, and returns a cosine similarity score. 
In mathematical terms, this score is the dot product of the two vectors divided 
by the product of their lengths.

The Jaccard similarity score is similar to the cosine similarity score, but only 
considers unique words per document instead of frequency. To calculate this score,
we first converted each vector into a logical vector, based on whether or not 
the term was present in the respective document. The Jaccard similarity score 
is then calculated as the intersection of the two vectors divided by the union 
of the two vectors.

The mean cosine and Jaccard similarity scores for each company, grouped by 
company status, are listed in the below table.

**Table 3 - similarity scores by company status**
```{r similarity, message = F, echo = F}

combined_cosine <- data_joined %>% 
  group_by(status) %>% 
  summarize(count = n(),
            entrepreneurship = mean(ent_c), finance = mean(fin_c), 
            leadership = mean(ldr_c), marketing = mean(mkt_c),
            strategy = mean(str_c)) %>% 
  kbl() %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position") %>% 
  add_header_above(c(" " = 1, " " = 1, "Mean cosine similarity scores" = 5))
combined_cosine

combined_jaccard <- data_joined %>% 
  group_by(status) %>% 
  summarize(count = n(),
            entrepreneurship = mean(ent_j), finance = mean(fin_j), 
            leadership = mean(ldr_j), marketing = mean(mkt_j),
            strategy = mean(str_j)) %>% 
  kbl() %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position") %>% 
  add_header_above(c(" " = 1, " " = 1, "Mean Jaccard similarity scores" = 5))
combined_jaccard

```

#### Data integration and regression analysis

Combining these sources of data gave us a table with X independent variables (the
cosine and Jaccard similarity scores for each company - how closely a company's 
description matched a business topic) and two dependent variables. The first 
dependent variable was logical: whether or not a company received funding. The 
second dependent variable was ordinal, based on a company's exit status: "Inactive",
"Active", "Acquired", or "Public", in that order. We ran two regressions using 
this data to determine if there was a relationship between the textual data and 
(1) whether a company received funding and (2) its viability as a company as 
measured by exit status.

## **Results**

It is difficult to draw robust conclusions from our two regressions. The results of 
each regression are listed in the below tables, with p-value .05 statistically significant 
independent variables in bold. 

For the first regression, similarity scores versus a company's exit status, the estimates
of the coefficient for each variable in the regression formula varied widely in both 
sign and value. Only one variable, the marketing cosine similarity score, was 
statistically significant at a p-value of .05. We could interpret this as with an 
one unit increase in the marketing cosine similarity score, the log odds of a company
progressing through each status increases by 0.30. However, due to the wide variety
in the rest of the results it is impossible to draw a solid conclusion from this.

**Table 4 - exit status regression table**
```{r status, message = F, echo = F}
require(broom)
# take polr output and calculate log odds and p-value
regr_table_status <- log_status %>% 
  tidy() %>% 
  mutate("log odds" = exp(estimate)) %>% 
  relocate("log odds", .after = estimate) %>% 
  mutate("p value" = pnorm(abs(statistic), lower.tail = F) * 2) %>%
  relocate("p value", .after = statistic) %>%
  rename("t statistic" = statistic) %>%
  kbl() %>% 
  kable_classic() %>% 
  kable_styling("striped", latex_options = "hold_position") %>% 
  row_spec(3, bold = T)
regr_table_status
```

For the second regression, similarity scores versus whether or not a company received
startup funding, the results were marginally more clear. Four variables, the 
entrepreneurship, finance, and marketing cosine similarity scores and the entrepreneurship
Jaccard similarity score had statistically significant estimates of their coefficient.
This could be interpreted as a one unit increase in these similarity scores corresponding
to a respective increase or decrease in the log odds of a company receiving funding. 
Again however, the wide variance in results makes it difficult to draw solid conclusions.
If there were truly a strong relationship, one would expect to see similar results 
for each cosine or Jaccard similarity score. Instead, they again vary greatly in
both sign and value, leading to inconclusive results.

**Table 5 - funded regression table**
```{r funded, message = F, echo = F}
require(broom)
regr_table_funded <- log_funded %>% 
  tidy() %>% 
  kbl() %>% 
  kable_classic() %>% 
  kable_styling("striped", latex_options = "hold_position", position = "left") %>% 
  row_spec(1:3, bold = T) %>% 
  row_spec(5, bold = T) %>% 
  row_spec(7, bold = T)
regr_table_funded
```

## **Conclusion**

text text text
text text tex

## **References**

### R libraries

We used the below libraries in our analysis and to develop this report:

tidyverse / tm / MASS / xml2 / rvest / urbnmapr / here / dplyr / broom / kableExtra

tidytext / textstem / sjPlot / pdftools / textclean / widyr / text2vec / lsa                    

### Company descriptions

All company descriptions were scraped from their respective pages at 
ycombinator.com/companies

### Textbook extracts

We extracted the table of contents, indices, and glossaries (availability varied
by textbook) from the below open source textbooks to build our dictionaries:

--Burnett, J. (n.d.). Introducing marketing. Open Textbook Library. Retrieved April 21, 2022, from https://open.umn.edu/opentextbooks/textbooks/introducing-marketing 

--Coleman, W., &amp; Halbardier, A. (n.d.). Principles of Management. openstax.org. Retrieved April 21, 2022, from https://openstax.org/details/books/principles-management 

--Laverty, M., &amp; Littel, C. (n.d.). Entrepeneurship. OpenStax. Retrieved April 21, 2022, from https://openstax.org/details/books/entrepreneurship 

--Reed, K. B. (n.d.). Strategic management. Open Textbook Library. Retrieved April 21, 2022, from https://open.umn.edu/opentextbooks/textbooks/mastering-strategic-management 

--Taylor, J., Robison, L., Hanson, S., &amp; Black, J. R. (2021, January 28). Financial Management for small businesses, 2nd Oer edition. Financial Management for Small Businesses 2nd OER Edition. Retrieved April 21, 2022, from https://openbooks.lib.msu.edu/financialmanagement/ 

### Funding data

Crunchbase funding data provided by Katie Moon, Professor of Finance at 
University of Colorado Boulder.

