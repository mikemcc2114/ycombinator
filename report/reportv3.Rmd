---
title: 'Words have meaning: language choice and startup success'
author: "Zachary Hayes, Justin Liu, Mike McCormick"
date: '2022-04-18'
header-includes:
    - \usepackage{setspace}\doublespacing
output:
  pdf_document:
    extra_dependencies: float
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "!H", out.extra = "", fig.width=6, fig.height=3)
```

```{r libraries, echo = F, warning = F, error = F, message = F, results = 'hide' }
library(tidyverse)
library(tm)
library(xml2)
library(rvest)
library(here)
library(dplyr)
library(tidytext)
library(textstem)
library(pdftools)
library(textclean)
library(text2vec)
library(lsa)
library(widyr)
library(kableExtra)
library(sjPlot)
library(wordcloud)
library(broom)
library(urbnmapr)
library(MASS)
```


```{r import data, echo = F, warning = F, error = F, message = F, results = 'hide' }

################################################################################
### scrape company descriptions

# Y Combinator website sitemap
sitemap <- "https://www.ycombinator.com/companies//sitemap.xml"
pattern <- "https://www.ycombinator.com/companies/"

# read sitemap XML and convert into dataframe and vector of company name and urls
company_url <- sitemap %>% read_xml() %>% as_list() %>% 
  as_tibble() %>% unnest_longer(urlset) %>% 
  filter(urlset_id == "loc") %>% 
  unnest(cols = names(.)) %>% unnest(urlset) %>% rename("url" = "urlset") %>% 
  mutate(company_name = gsub("https://www.ycombinator.com/companies/", "", url))

# write data to csv
#write_csv(company_url, file = here("data", "company_url.csv"))

# define paths for company name, season, and status tags
path_name <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[1]/h1"
path_season <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[2]/span[1]/text()"
path_status <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[2]/span[2]"

# scrape company descriptions from Y Combinator website based on url vector
# ycombinator_data <- tibble(directory_name = character(), 
#                            website_name = character(), season = character(), 
#                            status = character(), headline = character(), 
#                            description = character(), url = character())

# for(i in 1:length(company_url$url)){
#   cat("Iteration", i, "out of", length(company_url$url), "\n")
#   page <- read_html(company_url$url[i])
#   directory_name <- company_url$company_name[i]
#   website_name <- page %>% html_nodes(xpath = path_name) %>% html_text()
#   season <- page %>% html_nodes(xpath = path_season) %>% html_text()
#   status <- page %>% html_nodes(xpath = path_status) %>% html_text()
#   headline <- page %>% html_nodes("h3") %>% html_text() %>% .[1]
#   description <- page %>% html_nodes("p") %>% html_text() %>% .[1]
#   ycombinator_data <- ycombinator_data %>%  
#     add_row(directory_name = directory_name, website_name = website_name, 
#             season = season, status = status, headline = headline,
#             description = description, url = company_url$url[i])
# }

# write data to csv

# read previously downloaded data from csv
ycombinator_data <- read_csv(here("data", "ycombinator_data.csv"))

################################################################################
### company location information

# define path for company location
path_location <- "/html/body/div/div[1]/div/div/div[2]/div/div[2]/div[3]/span[2]"

# scrape company locations from Y Combinator website based on url vector
location_data <- tibble(directory_name = character(), location = character())

# for(i in 1:length(company_url$url)){
#    cat("Iteration", i, "out of", length(company_url$url), "\n")
#    page <- read_html(company_url$url[i])
#    directory_name <- company_url$company_name[i]
#    location <- page %>% html_nodes(xpath = path_location) %>% html_text()
#    location_data <- location_data %>%
#      add_row(directory_name = directory_name, location = location)
# }

# write data to csv
#write_csv(location_data, file = here("data", "location_data.csv"))

# read data from csv
location_data <- read_csv(here("data", "location_data.csv"))

# summary table
location_summary <- location_data %>% 
  count(location)

# write data to csv, then manually categorize
#write_csv(location_summary, file = here("data", "location_summary.csv"))

# read manually categorized summary
location_summary <- read_csv(here("data", "location_summary.csv"))

# break down data by country
country_breakdown <- location_summary %>%
  group_by(country) %>% 
  summarize(count = sum(n))

# top 5 countries
top_5_countries <- country_breakdown %>%
  slice_max(count, n = 5) %>% 
  mutate(category = "country") %>% 
  rename(location = country)

# top_5_countries_plot <- ggplot(top_5_countries, aes(x = reorder(country, -count), y = count)) +
#   geom_col() +
#   labs(x = "country", title = "Top five countries where Y Combinator companies are founded")

# break down data by US states
state_breakdown <- location_summary %>%
  filter(country == "USA") %>% 
  group_by(state) %>% 
  summarize(count = sum(n))

# top 5 states
top_5_states <- state_breakdown %>%
  slice_max(count, n = 5) %>% 
  mutate(category = "state") %>% 
  rename(location = state)

# top_5_states_plot <- ggplot(top_5_states, aes(x = reorder(state, -count), y = count)) +
#   geom_col() +
#   labs(x = "state", title = "Top five states where Y Combinator companies are founded")

# combined states and countries data
combined_location <- top_5_states %>% 
  add_row(top_5_countries)

combined_location_plot <- ggplot(combined_location, 
                                 aes(x = reorder(location, -count), y = count)) +
  geom_col() +
  facet_grid(~category, scales = "free_x") +
  labs(title = "Figure 2 - top five locations where companies are founded", x = "",
       y = "number of companies") + 
  theme(axis.text.x = element_text(angle = 45, vjust = .9 , hjust=1))

################################################################################
### state heatmap

# rename tibble to match ubrnmapr format
heatmap_states <- state_breakdown %>% 
  rename(state_abbv = state) 

# read state geo data
us_st <- urbnmapr::states 

# join data
company_st <- left_join(us_st, heatmap_states, by = "state_abbv")
company_st_plot <-  company_st %>%
  ggplot(aes(long, lat, group = group, fill = count)) +
  geom_polygon(color = NA) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(title = "Figure 3 - geographic distribution of Y Combinator companies",
       x = "", y = "", ) +
    theme(axis.title.x=element_blank(), 
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(), 
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())

################################################################################
### import textbook extracts

pdf_list <- list.files(here("data", "textbooks"))
textbooks <- tibble(textbook = character(), text = character())

for(i in 1:length(pdf_list)){
  cat("Iteration", i, "out of", length(pdf_list), "\n")
  textbook <-  pdf_list[i]
  text <- here("data", "textbooks", pdf_list[i]) %>%
    pdf_text() %>% str_flatten()
  textbooks <- textbooks %>% add_row(textbook = textbook,
                                     text = text)
}

# remove extraneous title info and rename columns to match company tibble
textbooks <- textbooks %>% 
  mutate(textbook = textbook %>% 
           str_remove_all("extract_|.pdf")) %>% 
  rename(directory_name = textbook, description = text)

################################################################################
### descriptive statistics

# ycombinator initial data

yco_init <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
    rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  group_by(document) %>% 
  summarize(count_words = n())

yco_init_stats <- yco_init %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "companies, pre-processing") %>% 
  relocate(data)

# ycombinator post-processing

remove <- "[:digit:]|[:blank:]|[:punct:]"

yco_post <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word)) %>% 
  group_by(document) %>% 
  summarize(count_words = n()) 

yco_post_stats <- yco_post %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "companies, post-processing") %>% 
  relocate(data)

# textbook initial data

textbooks_init <- textbooks %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  group_by(document) %>% 
  summarize(count_words = n())

textbooks_init_stats <- textbooks_init %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "textbooks, pre-processing") %>% 
  relocate(data)

# textbooks post-processing

remove <- "[:digit:]|[:blank:]|[:punct:]"

textbooks_post <- textbooks %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word)) %>% 
  group_by(document) %>% 
  summarize(count_words = n()) 

textbooks_post_stats <- textbooks_post %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "textbooks, post-processing") %>% 
  relocate(data)

# combined summary table

descr_stats <- yco_init_stats %>% 
  add_row(yco_post_stats) %>% 
  add_row(textbooks_init_stats) %>% 
  add_row(textbooks_post_stats) 

descr_stats_table <- kbl(descr_stats) %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position") %>% 
  kable_styling(latex_options = "scale_down") %>% 
  add_header_above(c(" " = 1, " " = 1, "words per document" = 7))

# combined histogram

yco_init <- yco_init %>% 
  mutate(data = "before processing")

yco_post <- yco_post %>% 
  mutate(data = "after processing")

yco_init_post <- yco_init %>% add_row(yco_post)
yco_init_post$data <- factor(yco_init_post$data, 
                             levels = c("before processing",
                                        "after processing"))

yco_hist <- yco_init_post %>% 
  ggplot(aes(x = count_words)) +
  geom_histogram() +
  facet_grid(~data) + 
  labs(x = "words per company description", y = "frequency",
       title = "Figure 1 - histogram of words per company description")
 

################################################################################
### combine company descriptions and textbook extracts, initial processing to 
### trim whitespace and NA values

data <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  filter(!is.na(description)) %>%
  filter(description != "") %>% 
  add_row(textbooks) %>% 
  rename(document = directory_name, text = description)

################################################################################
### tidy and clean documents

# unnest words, remove numbers/whitespace/punctuation/blank rows/stopwords,
# lemmatize

remove <- "[:digit:]|[:blank:]|[:punct:]"

data_clean <- data %>% 
  dplyr::select(document, text) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word))

data_clean_companies <- data_clean %>% 
  count(document)

# calculate term frequency per company
tf <- data_clean %>% 
  count(document, word, sort = TRUE)

# calculate total terms per company  
total_words <- tf %>% 
  group_by(document) %>% 
  summarise(total = sum(n))

# join tables together
tf <- left_join(tf, total_words)

# cast into dtm format and convert to matrix
dtm <- tf %>% 
  cast_dtm(document, word, n)
inspect(dtm)

dtm_matrix <- as.matrix(dtm)

# tf-idf table

tf_idf <- tf %>% 
  bind_tf_idf(word, document, n)

tf_idf_sparse <- tf_idf %>% 
  cast_sparse(document, word, tf)

################################################################################
### calculate cosine similarity per textbook and company description

# extract vectors for each textbook
x_ent <- dtm_matrix["entrepeneurship",]
x_fin <- dtm_matrix["finance",]
x_ldr <- dtm_matrix["leadership",]
x_mkt <- dtm_matrix["marketing",]
x_str <- dtm_matrix["strategy",]

# convert to jaccard logical vectors for each textbook
x_ent_j <- x_ent > 0
x_fin_j <- x_fin > 0
x_ldr_j <- x_ldr > 0 
x_mkt_j <- x_mkt > 0
x_str_j <- x_str > 0

# create empty cosine tibble
cosine_table <- tibble(directory_name = character(), ent_c = numeric(), 
                       fin_c = numeric(), ldr_c = numeric(), mkt_c = numeric(),
                       str_c = numeric(), ent_j = numeric(), fin_j = numeric(),
                       ldr_j = numeric(), mkt_j = numeric(), str_j = numeric())

# define jaccard similiary function
jaccard <- function(x, y) {
  intersection = x %*% y
  union = length(x) + length(y) - intersection
  return(intersection/union)
}

# calculate cosine and jaccard similarity for each company and textbook
for(i in 1:length(data_clean_companies$document)){
  cat("Iteration", i, "out of", length(data_clean_companies$document), "\n")
  doc <-  data_clean_companies$document[i]
  y <- dtm_matrix[doc,]
  y_j <- y > 0

  #calculate cosine similarity
  ent_c <- cosine(x_ent, y)
  fin_c <- cosine(x_fin, y)
  ldr_c <- cosine(x_ldr, y)
  mkt_c <- cosine(x_mkt, y)
  str_c <- cosine(x_str, y)

  #calculate jaccard similiary
  ent_j <- jaccard(x_ent_j, y_j)
  fin_j <- jaccard(x_fin_j, y_j)
  ldr_j <- jaccard(x_ldr_j, y_j)
  mkt_j <- jaccard(x_mkt_j, y_j)
  str_j <- jaccard(x_str_j, y_j)

  cosine_table <- cosine_table %>%  
    add_row(directory_name = doc, ent_c = ent_c, fin_c = fin_c,
            ldr_c = ldr_c, mkt_c = mkt_c, str_c = str_c, ent_j = ent_j,
            fin_j = fin_j, ldr_j = ldr_j, mkt_j = mkt_j, str_j = str_j)
}

# remove textbooks from cosine_table and rename columns
remove <- c("entrepeneurship", "finance", "leadership", "marketing", "strategy")

cosine_table <- cosine_table %>% 
  filter(!directory_name %in% remove) %>% 
  mutate(ent_c = ent_c[,1], fin_c = fin_c[,1], ldr_c = ldr_c[,1],
         mkt_c = mkt_c[,1], str_c = str_c[,1])

################################################################################
### import and clean crunchbase funding data

# import funding data csv and add company name column
crunchbase_data <- read.csv(here("data", "CB_funding.csv")) %>% 
  mutate(directory_name = gsub("\\d*_", "", filename)) %>% 
  mutate(directory_name = gsub(".json", "", directory_name))

# filter based on matching name from Y Combinator website, collapse to one row
# per company
funding_data <- crunchbase_data %>% 
  dplyr::select(directory_name, num_funding_rounds) %>% 
  filter(directory_name %in% company_url$company_name) %>% 
  group_by(directory_name, num_funding_rounds) %>% 
  summarise(n = n_distinct(directory_name)) %>% 
  rename(funded = n)

################################################################################
### create dummy variables for exit status from ycombinator data

exit <- ycombinator_data %>% 
  dplyr::select(directory_name, status) %>% 
  mutate(acquired = ifelse(status == "Acquired", 1, 0)) %>% 
  mutate(active = ifelse(status == "Active", 1, 0)) %>% 
  mutate(inactive = ifelse(status == "Inactive", 1, 0)) %>% 
  mutate(public = ifelse(status == "Public", 1, 0))

################################################################################
### join similarity, funding, and exit tables

# join cosine table with funding data
join1 <- left_join(cosine_table, funding_data, by = "directory_name")

# join status table (exit info)
data_joined <- left_join(join1, exit, by = "directory_name")

# replace NA values with 0
data_joined$num_funding_rounds <- data_joined$num_funding_rounds %>% 
  replace_na(0)
data_joined$funded <- data_joined$funded %>% 
  replace_na(0)

################################################################################
### funding data statistics

funding_summary <- data_joined %>% 
  summarize(total = n(),
            active = sum(active),
            funded = sum(funded),
            inactive = sum(inactive),
            acquired = sum(acquired),
            public = sum(public)) %>% 
  pivot_longer(everything(), names_to = "status", values_to = "count")

funding_summary_plot <- ggplot(funding_summary, 
                               aes(x = reorder(status, -count), y = count)) +
  geom_bar(stat = "identity") +
  labs(x = "status", y = "number of companies",
       title = "Figure 4 - company status statistics")


################################################################################
### regression analysis

# ordinal logistic regression of company status

data_polr <- data_joined
data_polr$status <- factor(data_polr$status, 
                           levels = c("Inactive", "Active", "Acquired", "Public"), ordered = TRUE)

log_status_cosine <- polr(status ~ ent_c + fin_c + ldr_c + mkt_c + str_c,
                          data = data_polr, Hess = TRUE)

log_status_jaccard <- polr(status ~ ent_j + fin_j + ldr_j + mkt_j + str_j, 
                           data = data_polr, Hess = TRUE)

# multivariable logistic regression for funding

# funded multivariable logistic regression
log_funded_cosine <- glm(formula = funded ~ ent_c + fin_c + ldr_c + mkt_c + str_c, 
                         data = data_joined, family = "binomial"(link = "logit"))

log_funded_jaccard <- glm(formula = funded ~ ent_j + fin_j + ldr_j + mkt_j + str_j,
                          data = data_joined, family = "binomial"(link = "logit"))

```

## **Introduction**

#### Motivation
Startups matter. Compared to big companies, startups contribute the majority of
the net new jobs every year, and they create new opportunities for the whole society.
Large companies tend to invest in incremental technologies, but start-ups often 
invest in subversive innovation while carrying significantly more uncontrollable risks 
and unpredictable returns. They extend the productivity frontier of companies and
society and reap large rewards in return. A better understanding of the factors
that result in startup company success is invaluable to future economic growth.

#### Background
At the same time, startup companies are fraught with risk from having no reliable
method for assessing the future success of an unproven company. They tend to lack initial
investment before any revenue is generated by a possible product or service that 
would be offered, and other common issues such as short-term cash flows, high 
expenses, weak marketing and financial systems. Launching successful startups is
a complex endeavor that depends upon many factors. 

Y Combinator is a startup accelerator that invests in startups early on and helps
them grow and take off. Since 2005, YC has invested in over 3,000 companies that
are worth over $400B combined, including Airbnb, Instacart, Coinbase, Dropbox, 
and Reddit. YC passionately shares their knowledge and experience to entrepreneurs
worldwide as a startup educational institution. 

#### Hypothesis
Our paper attempts to determine if there is a relationship between the way a company
initially describes itself and subsequent success, as measured by funding and 
successful exit. This paper integrates data from three sources (Y Combinator, 
Crunchbase, and open source textbooks) and combines textual and regression analysis 
to determine if such a relationship exists.

We perform our research with the hypothesis that the language a company uses to 
describe itself in its initial stages is an indicator of later success. Specifically,
the degree to which a company describes itself with business language is related
to startup funding and successful exit.

## **Data**

To test our hypothesis, we integrated three different data sets in order to maintain
integrity and to have well-rounded data. RStudio was the program deployed to pull
the words that we used as the data as well as regex was utilized to clean the less
meaningful words out, leaving the actual information needed to test. The three sets
used were company descriptions from YCombinator, open-sourced business text books
as dictionaries and then funding data from CrunchBase.

#### Y Combinator startup company descriptions
The place we started was with Y Combinator’s website. This is where we pulled each
specific and unique company description from, that have participated in at least
one incubation phase with them. This process utilized a for loop, to run to the 
site and grab each description systematically one by one placing them in a vector.
Followed by cleaning the descriptions with regex which took out filler words, 
punctuation and symbols that took away clarity with regards to the data. This left
a nice clean data set that we were able to use.

Gathering the different descriptions, it is clear there are some strengths and 
limitations that should be taken into consideration. A strength for this data set
is that it is very large in the context of time. It is for all of the companies 
that went through an incubation phase and it dates all the way back to 2005. This
shows growth and consistency. The time frame also shows data from different economic
conditions like the 2008 crash to better conditions like after 2014 giving a broader
picture to the data. A few limitations are that this data set focuses mainly on 
start-ups as well as the majority of the companies founded in California. This 
could lead the data to have a skew due to these limitations since it is from a 
specific time in a business’s life and the companies were mainly located in a 
specific area. With all of that the company descriptions from Y Combinator are good
to use. The below figure breaks down the distribution of words per company description, 
before and after textual analysis processing.

```{r company data, message = F, echo = F}
yco_hist 
```

In addition to textual data, we were able to extract founding location data for the
majority of Y Combinator companies, depicted in the below two figures. This data set is 
heavily skewed towards companies founded in the United States, specifically California.

```{r location data, message = F, echo = F}
combined_location_plot 

company_st_plot
```

#### Open source textbook extracts
The second data set used were 5 different open source textbooks that focused on 
specific business topics like finance and management. These were chosen because 
they would allow us to test and see if the company descriptions had specific business
focuses in them allowing to have measurable differences in the descriptions. Trying
to find dictionaries that would allow us to find something interpretable was difficult
until we realized how powerful it would be if we could come up with our own that 
were in essence bias free. We chose business books with specific focuses and turned
the important book specific words inside them into the dictionary.

Turning the business textbooks into dictionaries was very useful for our test. 
Another for loop was used to go to each website and pull specific parts of the books
and put them in a vector. To narrow down the words we used from the books we decided
that the index, glossary and table of contents would have the most useful words in
the book. Again, followed by utilizing regex again and taking the filler words, 
punctuation and symbols out leaving clean dictionaries to use as data sets. This
leads to some strengths and limitations to consider. The limitations are that the
data sets could be considered small since only the index, glossary and table of 
content were used due to memory constraints. As well as the text books were open 
sourced and not peer reviewed publications from prominent schools. However, the 
strength of this data set is that it is clean and allows for easy interpretable 
data, making it perfect for this test.

The below table lists summary statistics for our two types of textual data, both
before and after textual analysis processing.

**Table 1 - textual data summary statistics**
```{r textual data, message = F, echo = F}
descr_stats_table
```

#### Crunchbase startup funding data
The final data set is the funding data from Crunchbase. Professor Katie Moon 
kindly provided us with this data, which is from a platform that aggregates
information about private and public companies. This funding data tells us the 
amounts, rounds and seasons that the companies were funded through Y Combinator. 
We used this because it is a great indicator to determine the varying level of 
success. The ultimate success being that the company went public but it also 
shows when they only went through a few seasons. The below table and figure break
out the number of companies per each funding and exit status category. This is 
a good example of the so-called "Power Law" of venture capital - the vast majority of
returns come from a very small subset of companies. In this case, the very small number
of acquired (362) and public (15) companies versus the total number of companies 
that went through the incubator program (3251).

**Table 2 - funding and exit data for Y Combinator companies**
```{r funding, message = F, echo = F}

funding_stats_table <- kbl(funding_summary) %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position", position = "left") 
funding_stats_table

funding_summary_plot

```

## **Methodology**

Once we obtained the required data, our methodology followed three main steps: 
(1) processing the data into useable form, (2) calculating cosine and Jaccard 
similarity scores between each company description and each textbook, and (3)
use ordered and multivariate regressions to analyse the relationship between
the calculated similarity scores and company outcomes. These steps are discussed
in detail in the following sections.

#### Textual data

Our two separate sources of textual data required different means to extract and
then process into usable formats. We extracted the company descriptions using 
the rvest package, looping through each company's individual website and scraping 
the required information. For the five textbooks, we manually extracted just the 
table of contents, index, or glossary (as available, it varied per textbook) and
then imported them into R using the pdftools package.

The initial textual data, once imported into R, was still not in a format useful
for analysis. We followed several steps to convert the data into a usable format:

(1) We removed all numbers, whitespace, punctuation, and any blank or "NA"
rows to reduce the data to just words. 

(2) Next, we "lemmatized" each word using the textstem::lemmatize_words
function. This function uses a dictionary based on the Mechura 2016 English 
lemmatization list. This step reduced the words to common base forms, reducing the 
complexity of the data and making it easier to analyze.

(3) Finally, we calculated term frequencies for each document, and then used this 
information to build a document-term matrix of all the company descriptions, 
textbook extracts, and terms. The matrix contains a set of vectors for each textbook
extract and company description, with each value corresponding to a specific term
and the frequency it is found in each document.

#### Funding and exit data

We based the current status of each company based on information scraped from the 
Y Combinator website - each company is described as either "Inactive" (gone out
of business), "Active", "Acquired", or "Public". We were fortunate in this project
in that the Y Combinator and Crunchbase websites use a similar naming convention 
for companies, making joining the textual data with the funding data an easy step.
We filtered the Crunchbase funding data down to the company name and the number 
of funding rounds received and joined this data to the list of companies using 
the dplyr::left_join function. The Crunchbase data only included companies that 
have received startup funding, enabling us to define a second logical variable 
for whether or not a company had received funding.

#### Similarity scores

The final processing step required was to calculate the similarity between each 
company description and the textbook extracts, i.e. how similar a company's 
initial descriptive language is to a business topic. We did this using two measures, 
the cosine and Jaccard similarity scores between each company description and 
each textbook. 

We calculated the cosine similarity score for each company description using the
lsa::cosine function. This function takes two arguments, the respective vectors 
for each company description and textbook, and returns a cosine similarity score. 
In mathematical terms, this score is the dot product of the two vectors divided 
by the product of their lengths.

The Jaccard similarity score is similar to the cosine similarity score, but only 
considers unique words per document instead of frequency. To calculate this score,
we first converted each vector into a logical vector, based on whether or not 
the term was present in the respective document. The Jaccard similarity score 
is then calculated as the intersection of the two vectors divided by the union 
of the two vectors.

The mean cosine and Jaccard similarity scores for each company, grouped by 
company status, are listed in the below table.

**Table 3 - similarity scores by company status**
```{r similarity, message = F, echo = F}

combined_cosine <- data_joined %>% 
  group_by(status) %>% 
  summarize(count = n(),
            entrepreneurship = mean(ent_c), finance = mean(fin_c), 
            leadership = mean(ldr_c), marketing = mean(mkt_c),
            strategy = mean(str_c)) %>% 
  kbl() %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position") %>% 
  add_header_above(c(" " = 1, " " = 1, "Mean cosine similarity scores" = 5))
combined_cosine

combined_jaccard <- data_joined %>% 
  group_by(status) %>% 
  summarize(count = n(),
            entrepreneurship = mean(ent_j), finance = mean(fin_j), 
            leadership = mean(ldr_j), marketing = mean(mkt_j),
            strategy = mean(str_j)) %>% 
  kbl() %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position") %>% 
  add_header_above(c(" " = 1, " " = 1, "Mean Jaccard similarity scores" = 5))
combined_jaccard

```

#### Data integration and regression analysis

Combining these sources of data gave us a table with X independent variables (the
cosine and Jaccard similarity scores for each company - how closely a company's 
description matched a business topic) and two dependent variables. The first 
dependent variable was logical: whether or not a company received funding. The 
second dependent variable was ordinal, based on a company's exit status: "Inactive",
"Active", "Acquired", or "Public", in that order. We ran two regressions using 
this data to determine if there was a relationship between the textual data and 
(1) whether a company received funding and (2) its viability as a company as 
measured by exit status.

## **Results**

It is difficult to draw robust conclusions from our two regressions. The results of 
each regression are listed in the below tables, with p-value .05 statistically significant 
independent variables in bold. 

For the first regression, similarity scores versus a company's exit status, the estimates
of the coefficient for each variable in the regression formula varied widely in both 
sign and value. Only one variable, the marketing cosine similarity score, was 
statistically significant at a p-value of .05. We could interpret this as with an 
one unit increase in the marketing cosine similarity score, the log odds of a company
progressing through each status increases by 0.30. However, due to the wide variety
in the rest of the results it is impossible to draw a solid conclusion from this.

**Table 4 - exit status regression table**
```{r status, message = F, echo = F}
require(broom)
# take polr output and calculate log odds and p-value
regr_table_status <- log_status %>% 
  tidy() %>% 
  mutate("log odds" = exp(estimate)) %>% 
  relocate("log odds", .after = estimate) %>% 
  mutate("p value" = pnorm(abs(statistic), lower.tail = F) * 2) %>%
  relocate("p value", .after = statistic) %>%
  rename("t statistic" = statistic) %>%
  kbl() %>% 
  kable_classic() %>% 
  kable_styling("striped", latex_options = "hold_position") %>% 
  row_spec(3, bold = T)
regr_table_status
```

For the second regression, similarity scores versus whether or not a company received
startup funding, the results were marginally more clear. Four variables, the 
entrepreneurship, finance, and marketing cosine similarity scores and the entrepreneurship
Jaccard similarity score had statistically significant estimates of their coefficient.
This could be interpreted as a one unit increase in these similarity scores corresponding
to a respective increase or decrease in the log odds of a company receiving funding. 
Again however, the wide variance in results makes it difficult to draw solid conclusions.
If there were truly a strong relationship, one would expect to see similar results 
for each cosine or Jaccard similarity score. Instead, they again vary greatly in
both sign and value, leading to inconclusive results.

**Table 5 - funded regression table**
```{r funded, message = F, echo = F}
require(broom)
regr_table_funded <- log_funded %>% 
  tidy() %>% 
  rename("z-statistic" = statistic) %>% 
  kbl() %>% 
  kable_classic() %>% 
  kable_styling("striped", latex_options = "hold_position", position = "left") %>% 
  row_spec(1:3, bold = T) %>% 
  row_spec(5, bold = T) %>% 
  row_spec(7, bold = T)
regr_table_funded
```

## **Conclusion**

The results of our analysis are inconclusive - we were unable to find evidence
for our hypothesis. The regression results varied widely across all variables and 
makes it impossible to draw strong inferences about a relationship between initial
word choice and subsequent startup success.

To improve on this analysis, future research should seek better sources of initial 
descriptive language. The company descriptions are likely somewhat skewed or biased 
in that they are meant for public consumption and advertising. In addition, these 
descriptions are limited in terms of textual data - the longest company description 
was 586 words before processing, and 287 after processing. A better representation
of initial descriptive language choice would be a dataset consisting of original applications 
for the Y Combinator incubator program. This would provide a far more accurate version
of choice of initial descriptive language than the proxy we used for our analysis.

In addition, a larger dataset could be built by incorporating applications from 
multiple incubator programs - e.g. TechStars. Due to processing and 
memory limitations, our dictionaries were limited to just the table of contents, 
glossary, and indices from business textbooks. More accurate dictionaries could be
built using entire textbooks or Wikipedia pages. This would provide a larger dataset
from which to build similarity scores.

## **References**

#### R libraries

We used the below libraries in our analysis and to develop this report:

tidyverse / tm / MASS / xml2 / rvest / urbnmapr / here / dplyr / broom / kableExtra

tidytext / textstem / sjPlot / pdftools / textclean / widyr / text2vec / lsa                    

#### Company descriptions

All company descriptions were scraped from their respective pages at the 
ycombinator.com/companies website.

#### Textbook extracts

We extracted the table of contents, indices, and glossaries (availability varied
by textbook) from the below open source textbooks to build our dictionaries:

--Burnett, J. (n.d.). Introducing marketing. Open Textbook Library. Retrieved April 21, 2022, from https://open.umn.edu/opentextbooks/textbooks/introducing-marketing 

--Coleman, W., &amp; Halbardier, A. (n.d.). Principles of Management. openstax.org. Retrieved April 21, 2022, from https://openstax.org/details/books/principles-management 

--Laverty, M., &amp; Littel, C. (n.d.). Entrepeneurship. OpenStax. Retrieved April 21, 2022, from https://openstax.org/details/books/entrepreneurship 

--Reed, K. B. (n.d.). Strategic management. Open Textbook Library. Retrieved April 21, 2022, from https://open.umn.edu/opentextbooks/textbooks/mastering-strategic-management 

--Taylor, J., Robison, L., Hanson, S., &amp; Black, J. R. (2021, January 28). Financial Management for small businesses, 2nd Oer edition. Financial Management for Small Businesses 2nd OER Edition. Retrieved April 21, 2022, from https://openbooks.lib.msu.edu/financialmanagement/ 

### Funding data

Crunchbase funding data provided by Katie Moon, Professor of Finance at 
University of Colorado Boulder.

