---
title: 'Words have meaning: initial descriptive language choice and startup success'
author: "Zachary Hayes, Justin Liu, Mike McCormick"
date: '2022-04-18'
output:
  html_document: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo = F, warning = F, error = F, message = F, results = 'hide' }
library(tidyverse)
library(tm)
library(xml2)
library(rvest)
library(here)
library(dplyr)
library(tidytext)
library(textstem)
library(pdftools)
library(textclean)
library(text2vec)
library(lsa)
library(widyr)
library(kableExtra)
library(sjPlot)
library(wordcloud)
library(broom)
library(urbnmapr)
library(MASS)
```


```{r import data, echo = F, warning = F, error = F, message = F, results = 'hide' }

################################################################################
### combine company descriptions and textbook extracts, initial processing to 
### trim whitespace and NA values

data <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  filter(!is.na(description)) %>%
  filter(description != "") %>% 
  add_row(textbooks) %>% 
  rename(document = directory_name, text = description)

################################################################################
### tidy and clean documents

# unnest words, remove numbers/whitespace/punctuation/blank rows/stopwords,
# lemmatize

remove <- "[:digit:]|[:blank:]|[:punct:]"

data_clean <- data %>% 
  dplyr::select(document, text) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word))

data_clean_companies <- data_clean %>% 
  count(document)

# calculate term frequency per company
tf <- data_clean %>% 
  count(document, word, sort = TRUE)

# calculate total terms per company  
total_words <- tf %>% 
  group_by(document) %>% 
  summarise(total = sum(n))

# join tables together
tf <- left_join(tf, total_words)

# cast into dtm format and convert to matrix
dtm <- tf %>% 
  cast_dtm(document, word, n)
inspect(dtm)

dtm_matrix <- as.matrix(dtm)

# tf-idf table

tf_idf <- tf %>% 
  bind_tf_idf(word, document, n)

tf_idf_sparse <- tf_idf %>% 
  cast_sparse(document, word, tf)

################################################################################
### calculate cosine similarity per textbook and company description

# extract vectors for each textbook
x_ent <- dtm_matrix["entrepeneurship",]
x_fin <- dtm_matrix["finance",]
x_ldr <- dtm_matrix["leadership",]
x_mkt <- dtm_matrix["marketing",]
x_str <- dtm_matrix["strategy",]

# convert to jaccard logical vectors for each textbook
x_ent_j <- x_ent > 0
x_fin_j <- x_fin > 0
x_ldr_j <- x_ldr > 0 
x_mkt_j <- x_mkt > 0
x_str_j <- x_str > 0

# create empty cosine tibble
cosine_table <- tibble(directory_name = character(), ent_c = numeric(), 
                       fin_c = numeric(), ldr_c = numeric(), mkt_c = numeric(),
                       str_c = numeric(), ent_j = numeric(), fin_j = numeric(),
                       ldr_j = numeric(), mkt_j = numeric(), str_j = numeric())

# define jaccard similiary function
jaccard <- function(x, y) {
  intersection = x %*% y
  union = length(x) + length(y) - intersection
  return(intersection/union)
}

# calculate cosine and jaccard similarity for each company and textbook
for(i in 1:length(data_clean_companies$document)){
  cat("Iteration", i, "out of", length(data_clean_companies$document), "\n")
  doc <-  data_clean_companies$document[i]
  y <- dtm_matrix[doc,]
  y_j <- y > 0

  #calculate cosine similarity
  ent_c <- cosine(x_ent, y)
  fin_c <- cosine(x_fin, y)
  ldr_c <- cosine(x_ldr, y)
  mkt_c <- cosine(x_mkt, y)
  str_c <- cosine(x_str, y)

  #calculate jaccard similiary
  ent_j <- jaccard(x_ent_j, y_j)
  fin_j <- jaccard(x_fin_j, y_j)
  ldr_j <- jaccard(x_ldr_j, y_j)
  mkt_j <- jaccard(x_mkt_j, y_j)
  str_j <- jaccard(x_str_j, y_j)

  cosine_table <- cosine_table %>%  
    add_row(directory_name = doc, ent_c = ent_c, fin_c = fin_c,
            ldr_c = ldr_c, mkt_c = mkt_c, str_c = str_c, ent_j = ent_j,
            fin_j = fin_j, ldr_j = ldr_j, mkt_j = mkt_j, str_j = str_j)
}

# remove textbooks from cosine_table and rename columns
remove <- c("entrepeneurship", "finance", "leadership", "marketing", "strategy")

cosine_table <- cosine_table %>% 
  filter(!directory_name %in% remove) %>% 
  mutate(ent_c = ent_c[,1], fin_c = fin_c[,1], ldr_c = ldr_c[,1],
         mkt_c = mkt_c[,1], str_c = str_c[,1])

################################################################################
### import and clean crunchbase funding data

# import funding data csv and add company name column
crunchbase_data <- read.csv(here("data", "CB_funding.csv")) %>% 
  mutate(directory_name = gsub("\\d*_", "", filename)) %>% 
  mutate(directory_name = gsub(".json", "", directory_name))

# filter based on matching name from Y Combinator website, collapse to one row
# per company
funding_data <- crunchbase_data %>% 
  dplyr::select(directory_name, num_funding_rounds) %>% 
  filter(directory_name %in% company_url$company_name) %>% 
  group_by(directory_name, num_funding_rounds) %>% 
  summarise(n = n_distinct(directory_name)) %>% 
  rename(funded = n)

################################################################################
### create dummy variables for exit status from ycombinator data

exit <- ycombinator_data %>% 
  dplyr::select(directory_name, status) %>% 
  mutate(acquired = ifelse(status == "Acquired", 1, 0)) %>% 
  mutate(active = ifelse(status == "Active", 1, 0)) %>% 
  mutate(inactive = ifelse(status == "Inactive", 1, 0)) %>% 
  mutate(public = ifelse(status == "Public", 1, 0))

################################################################################
### join similarity, funding, and exit tables

# join cosine table with funding data
join1 <- left_join(cosine_table, funding_data, by = "directory_name")

# join status table (exit info)
data_joined <- left_join(join1, exit, by = "directory_name")

# replace NA values with 0
data_joined$num_funding_rounds <- data_joined$num_funding_rounds %>% 
  replace_na(0)
data_joined$funded <- data_joined$funded %>% 
  replace_na(0)

################################################################################
### funding data statistics

funding_summary <- data_joined %>% 
  summarize(total = n(),
            active = sum(active),
            funded = sum(funded),
            inactive = sum(inactive),
            acquired = sum(acquired),
            public = sum(public)) %>% 
  pivot_longer(everything(), names_to = "status", values_to = "count")

funding_summary_plot <- ggplot(funding_summary, 
                               aes(x = reorder(status, -count), y = count)) +
  geom_bar(stat = "identity") +
  labs(x = "funding / status category", y = "number of companies",
       title = "Figure 4 - company funding and status statistics")


################################################################################
### regression analysis

# ordinal logistic regression of company status

data_polr <- data_joined
data_polr$status <- factor(data_polr$status, 
                           levels = c("Inactive", "Active", "Acquired", "Public"), ordered = TRUE)

log_status <- polr(status ~ ent_c + fin_c + ldr_c + mkt_c + str_c + ent_j + fin_j +
                ldr_j + mkt_j + str_j, data = data_polr, Hess = TRUE)

# multivariable logistic regression for funding

# funded multivariable logistic regression
log_funded <- glm(formula = funded ~ ent_c + fin_c + ldr_c + mkt_c + str_c +
                    ent_j + fin_j + ldr_j + mkt_j + str_j,
                  data = data_joined, family = "binomial"(link = "logit"))

```

## Introduction

text text text

### Motivation

text text text

### Background

text text text

### Hypothesis

text text text

## Data

### Y Combinator startup company descriptions

text text text

```{r descriptions, echo = F, warning = F, error = F, message = F, results = 'hide' }
################################################################################
### scrape company descriptions

# Y Combinator website sitemap
sitemap <- "https://www.ycombinator.com/companies//sitemap.xml"
pattern <- "https://www.ycombinator.com/companies/"

# read sitemap XML and convert into dataframe and vector of company name and urls
company_url <- sitemap %>% read_xml() %>% as_list() %>% 
  as_tibble() %>% unnest_longer(urlset) %>% 
  filter(urlset_id == "loc") %>% 
  unnest(cols = names(.)) %>% unnest(urlset) %>% rename("url" = "urlset") %>% 
  mutate(company_name = gsub("https://www.ycombinator.com/companies/", "", url))

# write data to csv
#write_csv(company_url, file = here("data", "company_url.csv"))

# define paths for company name, season, and status tags
path_name <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[1]/h1"
path_season <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[2]/span[1]/text()"
path_status <- "/html/body/div/div[2]/div/div/div[1]/div[1]/div[2]/div[2]/span[2]"

# scrape company descriptions from Y Combinator website based on url vector
# ycombinator_data <- tibble(directory_name = character(), 
#                            website_name = character(), season = character(), 
#                            status = character(), headline = character(), 
#                            description = character(), url = character())

# for(i in 1:length(company_url$url)){
#   cat("Iteration", i, "out of", length(company_url$url), "\n")
#   page <- read_html(company_url$url[i])
#   directory_name <- company_url$company_name[i]
#   website_name <- page %>% html_nodes(xpath = path_name) %>% html_text()
#   season <- page %>% html_nodes(xpath = path_season) %>% html_text()
#   status <- page %>% html_nodes(xpath = path_status) %>% html_text()
#   headline <- page %>% html_nodes("h3") %>% html_text() %>% .[1]
#   description <- page %>% html_nodes("p") %>% html_text() %>% .[1]
#   ycombinator_data <- ycombinator_data %>%  
#     add_row(directory_name = directory_name, website_name = website_name, 
#             season = season, status = status, headline = headline,
#             description = description, url = company_url$url[i])
# }

# write data to csv

# read previously downloaded data from csv
ycombinator_data <- read_csv(here("data", "ycombinator_data.csv"))

################################################################################
### scrape company location information

# define path for company location
path_location <- "/html/body/div/div[1]/div/div/div[2]/div/div[2]/div[3]/span[2]"

# scrape company locations from Y Combinator website based on url vector
location_data <- tibble(directory_name = character(), location = character())

# for(i in 1:length(company_url$url)){
#    cat("Iteration", i, "out of", length(company_url$url), "\n")
#    page <- read_html(company_url$url[i])
#    directory_name <- company_url$company_name[i]
#    location <- page %>% html_nodes(xpath = path_location) %>% html_text()
#    location_data <- location_data %>%
#      add_row(directory_name = directory_name, location = location)
# }

# write data to csv
#write_csv(location_data, file = here("data", "location_data.csv"))

# read data from csv
location_data <- read_csv(here("data", "location_data.csv"))

# summary table
location_summary <- location_data %>% 
  count(location)

# write data to csv, then manually categorize
#write_csv(location_summary, file = here("data", "location_summary.csv"))

# read manually categorized summary
location_summary <- read_csv(here("data", "location_summary.csv"))

# break down data by country
country_breakdown <- location_summary %>%
  group_by(country) %>% 
  summarize(count = sum(n))

# top 5 countries
top_5_countries <- country_breakdown %>%
  slice_max(count, n = 5) %>% 
  mutate(category = "country") %>% 
  rename(location = country)

# top_5_countries_plot <- ggplot(top_5_countries, aes(x = reorder(country, -count), y = count)) +
#   geom_col() +
#   labs(x = "country", title = "Top five countries where Y Combinator companies are founded")

# break down data by US states
state_breakdown <- location_summary %>%
  filter(country == "USA") %>% 
  group_by(state) %>% 
  summarize(count = sum(n))

# top 5 states
top_5_states <- state_breakdown %>%
  slice_max(count, n = 5) %>% 
  mutate(category = "state") %>% 
  rename(location = state)

# top_5_states_plot <- ggplot(top_5_states, aes(x = reorder(state, -count), y = count)) +
#   geom_col() +
#   labs(x = "state", title = "Top five states where Y Combinator companies are founded")

# combined states and countries data
combined_location <- top_5_states %>% 
  add_row(top_5_countries)

combined_location_plot <- ggplot(combined_location, 
                                 aes(x = reorder(location, -count), y = count)) +
  geom_col() +
  facet_grid(~category, scales = "free_x") +
  labs(title = "Figure 2 - top five countries/states where Y Combinator companies are founded", x = "",
       y = "number of companies") + 
  theme(axis.text.x = element_text(angle = 45, vjust = .9 , hjust=1))

################################################################################
### create state heatmap of companies

# rename tibble to match ubrnmapr format
heatmap_states <- state_breakdown %>% 
  rename(state_abbv = state) 

# read state geo data
us_st <- urbnmapr::states 

# join data
company_st <- left_join(us_st, heatmap_states, by = "state_abbv")
company_st_plot <-  company_st %>%
  ggplot(aes(long, lat, group = group, fill = count)) +
  geom_polygon(color = NA) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(title = "Figure 3 - geographic distribution of Y Combinator companies",
       x = "", y = "", ) +
    theme(axis.title.x=element_blank(), 
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(), 
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())
```

```{r company plots, message = F, echo = F}
yco_hist
combined_location_plot 
company_st_plot
```

### Open source textbooks

text text text

```{r textbook import, echo = F, warning = F, error = F, message = F, results = 'hide' }
################################################################################
### import textbook extracts

pdf_list <- list.files(here("data", "textbooks"))
textbooks <- tibble(textbook = character(), text = character())

for(i in 1:length(pdf_list)){
  cat("Iteration", i, "out of", length(pdf_list), "\n")
  textbook <-  pdf_list[i]
  text <- here("data", "textbooks", pdf_list[i]) %>%
    pdf_text() %>% str_flatten()
  textbooks <- textbooks %>% add_row(textbook = textbook,
                                     text = text)
}

# remove extraneous title info and rename columns to match company tibble
textbooks <- textbooks %>% 
  mutate(textbook = textbook %>% 
           str_remove_all("extract_|.pdf")) %>% 
  rename(directory_name = textbook, description = text)
```

```{r summary stats, echo = F, warning = F, error = F, message = F, results = 'hide' }
################################################################################
### descriptive statistics

# ycombinator initial data
yco_init <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
    rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  group_by(document) %>% 
  summarize(count_words = n())

# compute summary stats
yco_init_stats <- yco_init %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "companies, pre-processing") %>% 
  relocate(data)

# ycombinator post-processing
remove <- "[:digit:]|[:blank:]|[:punct:]"

# process company descriptions
yco_post <- ycombinator_data %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word)) %>% 
  group_by(document) %>% 
  summarize(count_words = n()) 

# compute summary stats
yco_post_stats <- yco_post %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "companies, post-processing") %>% 
  relocate(data)

# textbook initial data

textbooks_init <- textbooks %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  group_by(document) %>% 
  summarize(count_words = n())

textbooks_init_stats <- textbooks_init %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "textbooks, pre-processing") %>% 
  relocate(data)

# textbooks post-processing
remove <- "[:digit:]|[:blank:]|[:punct:]"

# process textbooks
textbooks_post <- textbooks %>% dplyr::select(directory_name, description) %>%
  mutate(description = str_trim(description, side = "both")) %>% 
  rename(document = directory_name, text = description) %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = replace_non_ascii(word)) %>% 
  mutate(word = str_replace_all(word, remove, "")) %>%
  filter(word != "") %>% 
  anti_join(stop_words) %>% 
  mutate(word = lemmatize_words(word)) %>% 
  group_by(document) %>% 
  summarize(count_words = n()) 

# compute summary stats
textbooks_post_stats <- textbooks_post %>% 
  summarize("document count" = n_distinct(document),
            minimum = min(count_words),
            "lower quartile" = quantile(count_words, .25),
            mean = mean(count_words),
            median = median(count_words),
            "upper quartile" = quantile(count_words, .75),
            maximum = max(count_words),
            "standard deviation" = sd(count_words)) %>% 
  mutate(data = "textbooks, post-processing") %>% 
  relocate(data)

# combined summary table
descr_stats <- yco_init_stats %>% 
  add_row(yco_post_stats) %>% 
  add_row(textbooks_init_stats) %>% 
  add_row(textbooks_post_stats) 

descr_stats_table <- kbl(descr_stats) %>% 
  kable_classic() %>% 
  add_header_above(c(" " = 1, " " = 1, "words per document" = 7))

# combined histogram
yco_init <- yco_init %>% 
  mutate(data = "before processing")

yco_post <- yco_post %>% 
  mutate(data = "after processing")

yco_init_post <- yco_init %>% add_row(yco_post)
yco_init_post$data <- factor(yco_init_post$data, 
                             levels = c("before processing",
                                        "after processing"))

yco_hist <- yco_init_post %>% 
  ggplot(aes(x = count_words)) +
  geom_histogram() +
  facet_grid(~data) + 
  labs(x = "words per company description", y = "frequency",
       title = "Figure 1 - histogram of words per company description")

```


**Table 1 - textual data summary statistics**
```{r textual data, message = F, echo = F}
descr_stats_table
```

### Crunchbase startup funding data

text text text

**Table 2 - funding and exit data for Y Combinator companies**
```{r funding, message = F, echo = F}

funding_stats_table <- kbl(funding_summary) %>% 
  kable_classic()
funding_stats_table

funding_summary_plot

```

## Methodology and results

### Methodology

Once we obtained the required data, our methodology followed three main steps: 
(1) processing the data into useable form, (2) calculating cosine and Jaccard 
similarity scores between each company description and each textbook, and (3)
use ordered and multivariate regressions to analyse the relationship between
the calculated similarity scores and company outcomes. These steps are discussed
in detail in the following sections.

#### Data processing

#### Similarity scores

text text text


**Table 3 - mean cosine and similarity scores by company status**
```{r similarity, message = F, echo = F}

combined_cosine <- data_joined %>% 
  group_by(status) %>% 
  summarize(count = n(),
            entrepreneurship = mean(ent_c), finance = mean(fin_c), 
            leadership = mean(ldr_c), marketing = mean(mkt_c),
            strategy = mean(str_c)) %>% 
  kbl() %>% 
  kable_classic() %>% 
  add_header_above(c(" " = 1, " " = 1, "Mean cosine similarity scores" = 5))
combined_cosine

combined_jaccard <- data_joined %>% 
  group_by(status) %>% 
  summarize(count = n(),
            entrepreneurship = mean(ent_j), finance = mean(fin_j), 
            leadership = mean(ldr_j), marketing = mean(mkt_j),
            strategy = mean(str_j)) %>% 
  kbl() %>% 
  kable_classic() %>% 
  add_header_above(c(" " = 1, " " = 1, "Mean Jaccard similarity scores" = 5))
combined_jaccard

```

#### Data integration and regression analysis

### Results

text text text

**Table 4 - exit status regression table**
```{r status, message = F, echo = F}
require(broom)
r_table_status <- log_status %>% tidy() %>% kbl() %>% kable_classic()
r_table_status
```

**Table 5 - funded regression table**
```{r funded, message = F, echo = F}
require(broom)
r_table_funded <- log_funded %>% tidy() %>% kbl() %>% kable_classic()
r_table_funded
```

## Conclusion

## References

### R libraries

### Data sources

#### Company descriptions

#### Textbooks

fix these:

Entrepeneurship: openstax, Entrepeneurship
Finance: Robert C. Higgins, Analysis for Financial Management, 10th Edition
Leadership: Richard L. Daft, The Leadership Experience
Marketing: Introducing Marketing, Open Textbook Library
Strategy: Strategic Management, VA Tech

#### Funding data

Crunchbase funding data provided by Professor Katie Moon

